
day07




1.进程, 线程,协程 回顾复习

	多进程:密集CPU运算,
			通信困难,消耗资源大

	多线程:密集I/O复用(输入输出)
			资源共享一个进程;通信简单
			multithreading.dummy   dummy模块	

	协程(gevent)是其他语言中没有的概念,python特有



2.豆瓣电影爬虫

	ol 是有序列表
	ul是什么列表?

	在检查页面 copy  xpath
		父元素平级的 删掉[] 下标,li[1],就能选中所有li标签元素!

		-----------------------------------------------------
		操作:
	        self.base_url = 'https://movie.douban.com/top250?filter=&start='

	           def analysis(self,data):
		        # 方式一:1.:解析数据(汤美丽)
		        # 转换类型
		        # soup = BeautifulSoup(data, 'lxml')
		        # # 2 解析
		        # movie_name_list = soup.select('.grid_view .title')

		        # 2.方式二:使用 lxml解析数据
		        html_data = etree.HTML(data)
		        movie_name_list = html_data.xpath('//*[@id="content"]/div/div[1]/ol/li/div/div[2]/div[1]/a/span[1]/text()')

		        # 返回的是个list 需要遍历取出 电影名
		        for name in movie_name_list:
		            # print(name.get_text()) # 方式一
		            print(name)   # 方式二
		 
		   注:用两种方式来解析获取的数据 ,方式二:其中选中电影名字,然后 反键检查,copy-->xpath-->xpath 插件
		   		这种是使用etree 来解析数据,返回的是个list,记得在后面 加上 '/text()' 取出内容;
		   		方式一: bs4 汤美丽:使用层级选择器 选中标签,返回的是个对象list,变量取值的时候 name.get_text()取出名字


		-----------------------------------------------------

	2.1单线程
		    def //*[@id="content"]/div/div[1]/ol/li/div/div[2]/div[1]/a/span[1]run(self):
		        # 1.记录开始时间
		        start_time = time.time()
		        # 总共 10页,循环发请求
		        for page in range(0, 225+1, 25):
		            print('正在下载第{}页...'.format(page/25))
		            url = self.base_url + str(page)
		            # 1.发送请求
		            data = self.send(url)
		            self.analysis(data)
		        # 2.记录结束时间
		        end_time = time.time()

		        # 3.计算耗时
		        delta = end_time - start_time

		        print('总共消耗的时间是:{}'.format(delta))
		        print('总共的电影有:{}'.format(self.count))

		  注:1.格式化输出: '{}'.format(xx)        
			2.单线程,循环发请求,记录算出总共消耗时间:总共消耗的时间是:24.6789999008s	


	2.2多线程

		    def run(self):
		        # 1.记录开始时间
		        start_time = time.time()

		        thread_list = []
		        for page in range(0,225+1,25):
		            print("正在下载第{}页...".format(page))
		            url = self.base_url + str(page)

		            # 1.创建线程
		            custom_thread = threading.Thread(target=self.send,args=[url])
		            # 2.启动
		            custom_thread.start()
		            thread_list.append(custom_thread)

		        # 3.让主线程等待,所有的子线程执行完毕
		        for thread_child in thread_list:
		            thread_child.join()

		        # 2.记录结束时间
		        end_time = time.time()

		        # 3.计算耗时
		        delta = end_time - start_time


			注:1.创建多线程只能指定一个函数send(),那么 analysis()解析函数就要
				放在send()函数里面调用!
				2.多线程总共:总共耗时:2.90400004387s


	2.3dummy

		    def run(self,):

		        url_list = []
		        # 1.记录开始时间
		        start_time = time.time()
		        for page in range(0, 225+1, 25):
		            print("正在下载第{}页...".format(page))
		            url = self.base_url + str(page)
		            url_list.append(url)
		        # 1.创建线程池,开很多的线程,总共有10次请求,那么就开10个进程池
		        pools = Pool(len(url_list))

		        # 2.告诉进程池该执行哪个任务
		        pools.map(self.send, url_list) # map()函数将url_list取出,放在send()中执行

		        # 3.关闭线程池
		        pools.close()

		        # 4.让主线程等待子线程执行完毕
		        pools.join()

		        # 2.记录结束时间
		        end_time = time.time()

		        # 3.计算耗时
		        delta = end_time - start_time
		        print("总共耗时:%s" % delta)
		        print("总共的电影:%s" % self.count)


			注: 1.
				2.总共耗时:3.51600003242


	2.4gevent(协程)

		    def run(self):
		        # 1.记录开始时间
		        start_time = time.time()
		        gevent_list = []

		        for page in range(0, 225+1, 25):
		            print("正在下载第{}页...".format(page))
		            url = self.base_url + str(page)

		            # 1.创建协程
		            gevents = gevent.spawn(self.send, url)

		            # 2.gevent_list
		            gevent_list.append(gevents)

		        # 3.主线程等待
		        gevent.joinall(gevent_list)

		        # 2.记录结束时间
		        end_time = time.time()

		        # 3.计算耗时
		        delta = end_time - start_time
		        print("总共耗时:%s" % delta)
		        print("总共的电影:{}".format(self.count))


		注:1.需要安装  VCForPython2.7来运行greenlet模块!       
			2.总共耗时:3.97000002861


	
3.	动态HTML和机器图像识别

		1.JavaScript


		2.jQuery入口:

		<script type="text/javascript" src="https://statics.huxiu.com/w/mini/static_2015/js/jquery-1.11.1.min.js?v=201512181512"></script>


		3.Ajax
			全称是 Asynchronous JavaScript and XML(异步 JavaScript 和 XML)
				网站不需要使用单独的页面请求就可以和网络服务器进行交互 (收发信息)。

		4.DHTML
			动态 HTML(Dynamic HTML, DHTML)也是一系列用于解决网络问题的 技术集合
				DHTML 是用客户端语言改变页面的 HTML 元素(HTML、CSS，或者二者皆 被改变)


		在python中采集动态数据

		只有两种途径:

			1.直接从 JavaScript 代码里采集内容（费时费力）
			2.用 Python 的 第三方库运行 JavaScript，直接采集你在浏览器里看到的页面（这个可以有）。


	1.selenium
		pip install selenium  ---->在Ubuntu16.04中可以使用命令安装：sudo apt-get install phantomjs
		如果其他系统无法安装，可以从它的官方网站http://phantomjs.org/download.html) 下载。

		selenium是一个Web的自动化测试工具,Selenium 可以直接运行在浏览器上
		
		Selenium 可以根据我们的指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏，或者判断网站上某些动作是否发生

		Selenium 库里有个叫 WebDriver 的 API,它也可以像 BeautifulSoup 或者其他 Selector 对象一样用来查找页面元素，与页面上的元素进行交互 (发送文本、点击等)，以及执行其他动作来运行网络爬虫。


	ORC库概述:

		Tesseract  :是一个 Python 的命令行工具
		安装:pip install pytesseract

		============================================================
	如果WEB页面被反爬了,那就看 app端 和HTML5




4.selenium的使用

	-----------------------------------------------------------
	百度快照为例:

		# coding=utf-8

		from selenium import webdriver
		import time

		def selenium_base_use():

		    # 1.创建浏览器(对象)
		    browser = webdriver.PhantomJS()
		    # 2.填网站
		    browser.get('https://www.baidu.com')
		    # 3.查看快照(百度首页)
		    # browser.save_screenshot("06.baidu.png")
		    time.sleep(2)
		    print('111')
		    # 4.取内容
		    # data = browser.page_source

		    # # 5.点击 新闻按钮
		    news_btn = browser.find_element_by_name('tj_trnews') # 百度 '新闻'a 标签name=tj_trnews
		    news_btn.click()

		    # # 6.填写 搜索内容-->unicode字符
		    search_bar = browser.find_element_by_id('ww')
		    search_bar.send_keys(u'春运')
		    time.sleep(2)
		    print('222')
		    # # 7.点击 '百度一下' 按钮
		    baidu_btn = browser.find_element_by_class_name('btn')
		    baidu_btn.click()
		    #
		    # # 8.点击 第二条新闻 (copy xpath)(发现页面跳转到了新的页面)
		    first_news = browser.find_element_by_xpath('//*[@id="2"]/h3/a').click()

		    # # 9.获取浏览器上面的页面( 目前到这里 浏览器总共打开了两个页面,我们需要的是第二个页面,用下标取)
		    print(browser.window_handles)
		    time.sleep(2)
		    print('333')
		    # # 10.切换页面 (切换页面到新的页面里去 )
		    browser.switch_to_window(browser.window_handles[1])

		    # 11.打印 页面的标题
		    print(browser.title)
		    print(browser.current_url)
		    print(browser.get_cookies())

		    # 3.保存快照
		    browser.save_screenshot('06baidu3.png')
		    #
		    # # 12.关闭页面
		    browser.close()
		    #
		    # # 13.关闭浏览器
		    browser.quit()

		    print('ok')

		if __name__ =='__main__':
		    selenium_base_use()

		注:1.如果运行过程中报错,那么就是 没有找到对应的标签,找标签可以根据:
			 	find_element_by_name()
			 	find_element_by_class_name()
			 	find_element_by_id()
			 	find_element_by_xpath()  # 万一其他方式找不到,by_xpath最保险!
		 	获取当前页面数据资源: data = driver.page_source
		 	找到了之后: .click() 点击进入!
		 	搜索框:send_keys(u'中文')
		 	保存快照:save_screenshot('xxx.png')  # 输入其他格式(jpg)会警告warning
			防止反爬:可以time.sleep(2)

		2.有些网站会跳转到新窗口,我们可以通过代码实现窗口的切换:
			browser.switch_to_window(browser.window_handles[1])  # 第二个窗口,下标
			对于百度需要这么切换,对于其他如:360so,搜狗 等有些页面不需要切换跳转	

		3.其他报错:selenium 版本高于3.5的会报 让你更换google或者火狐,可以通过卸载老版本,安装新版本来解决	

		-----------------------------------------------------------




5.browser对象调用execute_script()方法: 执行js
	
	    # 1.创建浏览器对象
	    driver = webdriver.PhantomJS()
	    # 2.获取数据
	    driver.get('https://www.baidu.com')
	    # 3.修改 搜索框
	    search_js = 'var input_elemet = document.getElementById("kw");input_elemet.style.border = "2px solid green"'
	    time.sleep(2)
	    # 4.执行js代码
	    driver.execute_script(search_js)

	    # 5.将隐藏的'input'输入框 ,变为'显示状态'
	    hidden_btn_js = 'var baidu_btn = document.getElementById("su");baidu_btn.style.display = "none"'
	    driver.execute_script(hidden_btn_js) # 执行

	    driver.save_screenshot('07.baidu.png')

	注: 1.修改百度搜索框的样式,只要用 写一段js代码,赋值给变量,执行这个变量,快照截图   
		用driver 要加延迟,不然页面会挂



6.python调用js:execjs():
	pip install PyExecJs 安装这个库

		-----------------------------------------------------
		import execjs

		if __name__ =='__main__':
		    js_code = 'function sum(a,b) {return a+b}'

		    # 用python执行,获取执行的结果

		    # 1.转换类型,转换成python能认识的格式
		    js_type = execjs.compile(js_code)

		    # 2.调用js方法,获取执行结果
		    res = js_type.call('sum',10,15)

		    print(res)
		--------------------------------------------------------	    



7.selenium爬取斗鱼房间名,主播名,人气:

	    def __init__(self):
        self.base_url = 'https://www.douyu.com/directory/all'
        # 生成浏览器对象
        self.driver = webdriver.PhantomJS()
        self.count = 0

    def send(self):
        time.sleep(2)
        self.driver.get(self.base_url)
        # 获取页面数据
        data = self.driver.page_source
        return data

    def analysis(self, data):

        # 1.转换类型
        soup= BeautifulSoup(data, 'lxml')

        # 2.解析,
        # 2.1(获取所有的房间,返回所有房间的名字的列表,#live-list-content是整个div的id,.ellipsis是当前房间的名字的class
        room_list = soup.select('#live-list-content .ellipsis')

        # 2.2获取所有房间得主播名字
        zhubo_list = soup.select('.dy-name')

        # 2.3 获取所有房间人气
        renqi_list = soup.select('.dy-num')

        # 2.4 获取游戏名
        game_list = soup.select('.tag')

        list = []
        # 3.输出  zip
        for room, zhubo, renqi, game in zip(room_list, zhubo_list, renqi_list, game_list):

            print(room.get_text().strip())   # strip()去空格
            print(zhubo.get_text())
            print(renqi.get_text())
            print(game.get_text())
            self.count += 1





9.机器视觉与tesseract

	ORC库概述




xxx.selenium是一个Web的自动化测试工具(不是python独有的,python只有爬虫会用到),
	尽量不用,因为很慢

	

==============================================

小结:
	1.根据标签 获取此行内其他标签的属性:

	    res10 = soup.select('p[name="dromouse"]') # 属性选择器

	   	# 2,取属性
	    attr = res10[0].get('class')






































































































































































































