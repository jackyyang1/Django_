
Day11


	如果是js加载的页面,需要调用浏览器来爬取数据
	问:jsonpath怎么用?


AQI项目实战


1.AQI的项目介绍
	https://www.aqistudy.cn/historydata/

	用scrapy shell "地址" 进入网站,用response.body ,re正则匹配内部内容是不是有 城市"深圳"
	或者"2018-02",城市匹配得到,日期 和中间的部分匹配不到,所以这个网站是 用js动态加载得到的
	这个时候 就需要调用无界面浏览器来获取数据!


	注: 在具体城市的详情空气质量数据里 :加载数据都是靠 xxx.js 来加载的!

	----------------------------------------------------------
	操作shell:
		1.进入shell:(所有城市页面)
		scrapy shell "https://www.aqistudy.cn/historydata/"

		2.print(response)

		3.import re 
		 res = re.findall(r'深圳',response.body)
		 [xxx,xxx] -->返回一个列表,都是一个城市

		4. 进入shell: ('所选城市为"深圳",查询深圳城市,每个月份的数据能否获取到') 
		 scrapy shell "https://www.aqistudy.cn/historydata/monthdata.php?city=%E6%B7%B1%E5%9C%B3"
		 res = re.findall(r'2017-12',response.body)
		 [] -->返回一个空列表,表示这个页面数据是js加载页面

		右边的'2013年12月' 可以匹配到,但中间的 '2013-12-1'无法找到 ,说明是部分js加载
		>>> res = re.findall(r'2013年12月',response.body)
		>>> print(res)
		['2013\xe5\xb9\xb412\xe6\x9c\x88']

		>>> print(res[0].decode('utf-8'))
		2013年12月


	---------------------------------------------------------


2.Chromedriver的使用
	
    2.1安装
		Chromedriver的安装: 根据Chrome浏览器版本来安装,windows下安装需要配置环境变量
		淘宝镜像源: https://npm.taobao.org/mirrors/chromedriver/


	2.2shell测试: (创建一个谷歌浏览器对象,会自动打开浏览器的页面)
		
		0.进入python交互页面:

			from selenium import webdriver
			driver = webdriver.Chrome()

		1.访问百度: driver.get('http://www.baidu.com')
		
		2. 查看是否当前页面标题: print(driver.title)

		3.切换页面:
			driver.switch_to_window(driver.window_handles[1])
			print(driver.title)

		4.关闭第二个页面: driver.close()
		
		5.关闭浏览器: driver.quit() 



3.分析抓取步骤和xpath提取数据的规则写法
	
	1.首页的网址--城市列表 url
	 "https://www.aqistudy.cn/historydata/"

	2.月份的url
	 'https://www.aqistudy.cn/historydata/monthdata.php?city=%E6%B7%B1%E5%9C%B3'

	3.每天的网址 url
	 'https://www.aqistudy.cn/historydata/daydata.php?city=%E6%B7%B1%E5%9C%B3&month=2013-12	'

	 城市的url
	 '/html/body/div[3]/div/div[1]/div[2]/div[2]/ul/div[2]/li/a/@href'
	 注: 城市的url 选中城市的 名字 text(),然后模拟点击,浏览器会自动补全网址
	 		从而返回正确的 城市url


	 城市的名字
	 '/html/body/div[3]/div/div[1]/div[2]/div[2]/ul/div[2]/li/a/text()'

	 月份的url
	 'https://www.aqistudy.cn/historydata/' + '//tr/td[1]/a/@href'	

	4.数据的抓取

	取出所有行: tr :'//tr'

	删除第一个 tr 汉字(头)

	城市名字:
		city_name = 


	空气质量的数据:
		# 日期
		date = './td[1]/text()'
		# AQI
		aqi =
		# 空气质量
		level = 
		# pm2.5
		pm2_5 =		
		pm10 = 
		# 二氧化硫
		so2 = 
		# 一氧化碳
		co = 
		# 二氧化氮
		no2 = 
		# 臭氧
		o3 = 

	问:为什么 有时候 yield item 给的是引擎 ,有时候是给的 管道?

	操作:
		1.scrapy startproject AQI
		2.cd AQI
		  scrapy gensipder aqi_spider aqi.com 
  				
		3.aqi_spider.py中:
			class aqispider(scrapy.Spider):
				name=
				allowed_domians = []
				start_urls = []
				base_url = ''

			# 解析城市列表url
			def parse(self, response):
				# 城市的url( 返回所有城市的列表) 
				city_url_list = response.xpath('').extract()				
				# 城市的名字
				city_name_list = response.xpath('').extract()

				# 循环遍历发送城市url 请求,拼接城市url发送请求
				for name, link in zip(city_name_list, city_url_list):
					url = self.base_url + name
					yield scrapy.Request(url=url,callback=self.month_parse,meta={'city_name':name})

			# 解析月份的列表,提取每天的url
			def month_parse(self, response):
				# 月份的链接		
				month_links_list = response.xpath('').extract()
				for month_link in month_links_list:

					url = self.base_url + month_link

					city_name = response.meta['city_name']
					yield scrapy.Request(url=url,callback=self.day_parse,meta={'city_name':city_name})

			# 解析每天的数据	
			def day_parse(self, response):
				# 1.提取所有行的 tr 标签,每个tr 标签里面都包含多个td 内容标签					
				tr_list = response.xpath('//tr')

				# 2.删除第一行 tr 汉字(表头)
				tr_list.pop(0)

				# 3.遍历循环,将每天的数据存在 item中
				for tr in tr_list:
					item = AqiItem()

					# 城市名字
					item['city_name'] = response.meta['city_name']

		            # 空气质量的数据:
		            # 日期
		            item['date'] = tr.xpath('./td[1]/text()').extract_first()
		            # AQI
		            item['aqi'] = tr.xpath('./td[2]/text()').extract_first()
		            # 空气质量
		            item['level'] = tr.xpath('./td[3]/span/text()').extract_first()  # 或者是./td[3]//text()
		            # pm2.5
		            item['pm2_5'] = tr.xpath('./td[4]/text()').extract_first()
		            item['pm10'] = tr.xpath('./td[5]/text()').extract_first()
		            # 二氧化硫
		            item['so2'] = tr.xpath('./td[6]/text()').extract_first()
		            # 一氧化碳
		            item['co'] = tr.xpath('./td[7]/text()').extract_first()
		            # 二氧化氮
		            item['no2'] = tr.xpath('./td[8]/text()').extract_first()
		            # 臭氧
		            item['o3'] = tr.xpath('./td[9]/text()').extract_first()

		            yield item

					解析:
						1.将原始的url 获取城市列表, 并将城市url 获取到,然后拼接,返回给爬虫交给下一个函数         
					    2.解析月份的列表,提取每天的url,交给爬虫下一个函数 day_parse() 	
					    3.解析每天的数据,每个 //tr 标签里面都包含多个td(td对应的是想要的数据),
					    	循环遍历tr 列表,将数据以键值对的形式 存在 item中 返回给管道

					    注:1.第三个 td 标签里面 还包含一个 span 标签,要区别对待,如果没获取到这个字段的数据那就是每处理	
					    	2.获取 url 的时候是通过 @href来获取的:
						    	city_url_list = response.xpath('/html/body/div[3]/div/div[1]/div[2]/div[2]/ul/div[2]/li/a/@href').extract()
		

		4.pipeline.py 管道操作
			1.在管道中定义类存储 数据源信息在item中 (名字 和 爬取时间)
				
			2.将数据 存为 json格式文件的管道: 因为是写入文件有打开 和关闭的操作

			以上两个操作都要 return item ,而且方法是固定的  def  process_item()


		5.setting.py 开启管道	


		6.以上代码只能获取第一页数据,后面的具体 某一城市,某一月,某天的数据都是动态js加载的
			要单独写代码获取,此时就需要用到中间件,具体见下面!




4.通过中间件操作Chrome请求数据
	思路:
		
				middleware
			    url
		  引擎---------->  下载器
			| <----------
			|	response
			|
			|
		爬虫代码	

	1.通常情况下 是引擎将url 发送给下载器,进行数据的下载,然后下载器返回response给爬虫代码解析	
	2.在这里 我们使用引擎和 下载器中间的中间件来处理 url的request请求,返回数据response对象给引擎,
		引擎再将response 传给爬虫代码解析,从而绕过了下载器!自己吧数据就直接获取到了 

	-------------------------------------------------------------		
	操作:
		1.middleware.py中写 Chrome的中间件

		from selenium import webdriver

		class ChromeMiddleware(object):

		    def process_request(self, request, spider):
		        # 如果是首页的url(静态数据),不需要调用浏览器
		        if request.url != 'https://www.aqistudy.cn/historydata/':

		            # 1.获取浏览器对象
		            driver = webdriver.Chrome()

		            # 2.发送请求
		            driver.get(request.url)
		            # 加延迟
		            time.sleep(2)

		            # 3.获取渲染完毕的数据
		            html_data = driver.page_source

		            # 4.关闭浏览器

		            # 5.将获取的 response响应对象返回给引擎
		            return scrapy.http.HtmlResponse(url=request.url,request=request,body=html_data.encode('utf-8'),encoding='utf-8')

		2.在setting中 开启中间件 :注意改中间名字            
			开启爬虫: scrapy crawl aqi_spider
			此时会得到 aqi.json文件  验证里面的数据,由于每次请求都会调Chrome浏览器
			而且是可视化页面,代码里一定要quit(),

		BUG'	
		   fuck:'我的代码不知道哪里出问题了,就是调不出谷歌浏览器的driver,json数据也获取不到
				'老师的代码我用windows环境跑,第一次跑通了,删除json文件之后重新爬,又获取不到了!!!!'

			yes:解决了: 原因是 我开错中间件了! setting里有两个中间件,一个是spider_middleware(爬虫和引擎之间的中间件)我开的是这个,
						造成了运行爬虫,获取不到数据! 应该开启 引擎和下载器之间的中间件():
						DOWNLOADER_MIDDLEWARES = {
 						  'AQI.middlewares.ChromeMiddleware': 543,
						} 

		

		注:在中间件代码里 request.url 总共有多个url,一个是 base_url,
			一个是 base_url + city ,一个是 base_url+ city+ month ... day 等待
			但是只有后面的是js加载页面,前面的 base_url 是静态的 不需要调用我们自己写的中间件浏览器,
			所以要排除! !=

		-----------------------------------------------------------



5.AQI-->Crawl版本
	
	新建爬虫文件aqi_crawl.py,使用crawlSpider来爬取数据:

	---------------------------------------------------------
	操作:

	1.crawl爬虫代码:

		from scrapy.linkextractors import LinkExtractor
		from scrapy.spiders import CrawlSpider, Rule
		from AQI.items import AqiItem


		class AqiCrawl(CrawlSpider):
		    name = 'aqi_crawl'
		    allowed_domains = ['aqistudy.cn']
		    base_urls = 'https://www.aqistudy.cn/historydata/'
		    start_urls = ['https://www.aqistudy.cn/historydata/']

		    rules = (
		        # rule 提取链接 ,自动去重,提取的Link会自动拼接和补全
		        # Rule(1.解析对象, 2.回调函数 3,是否跟进)
		        # 注意点:符号需要转译

		        # 因为城市列表的内容不需要,所以callback不需要;默认follow=True
		        # 1.解析 城市列表
		        Rule(LinkExtractor(allow=r'monthdata\.php\?city=')),

		        # 2.解析 每月的url (只要出现callback,follow就为False)
		        # 因为通过月的url 已经拿到每天的数据,所以解析callback;默认follow=False
		        Rule(LinkExtractor(allow=r'daydata\.php\?city\='), callback='day_parse',follow=False),
		    )


		    # 3 .解析 每天的数据
		    def day_parse(self, response):
		    	....

			注:
			1.rule可以自动去重,并且提取的link会自动拼接和补全  如提取的是'monthdata\.php\?city='
				name它会自动在前面加上: 	https://www.aqistudy.cn/historydata/
			2.Rule()方法: Rule(1.解析对象, 2.回调函数 3,是否跟进)
			3.注意点:符号需要转译
			4.我们只在需要数据的页面才 callback='' 回调函数来进行解析
			5.def day_parse() 跟之前的爬虫是一样的,直接拿过来使用,但是城市的名字没有 meta={'city_name':name}
				没有meta传递,所以之前的方式拿不到,需要下面单独处理

		2.处理城市名字:

			--------------------------------------------------	
			>>> ss = u'2013年05月安阳空气质量指数日历史数据'
			>>> s = ss[8:-11]
			>>> print(s)
			安阳
			--------------------------------------------------
			操作:
				1.获取页面标题:
    		    title = response.xpath('//h2/text()').extract_first()

    		    2.切分获取城市名字
    		    item['city_name'] = city_name[8:-11]


    	3.处理城市名字第二种方式:
    		
    		-----------------------------------------------------	    
			>>> surl = 'https://www.aqistudy.cn/historydata/daydata.php?city=%E5%AE%89%E9%98%B3&month=2013-05'
			>>> a = surl.find('=')
			>>> print a
			52
			>>> b = surl.find('&')
			>>> print b
			71
			>>> c = surl[53:71]
			>>> print c
			%E5%AE%89%E9%98%B3  ---> 取出这个是经过 url编码之后的16进制数据,需要反解码回去成汉字

			>>> import urllib
			>>> ss = urllib.unquote(c) -->url解码
			>>> print ss
			瀹夐槼          --------> 在python2 中的'str'是'ascii'编码,在windows的cmd界面显示的是乱码

			>>> print ss.decode('utf-8')
			安阳
			>>> cc = ss.decode('utf-8')
			>>> type(cc)
			<type 'unicode'>  --> python2 中原'str'在gbk中乱码情况下,将其decode('utf-8'),使其变为unicode 类型,就可以显示了
    		

			>>> cc      ---------->unicode (python2)
			u'\u5b89\u9633'
			>>> ss      ----------> str (python2)
			'\xe5\xae\x89\xe9\x98\xb3'

    		-----------------------------------------------------

    		注:
    		1.在详情页这个最终页面里面上面的地址里面其实是有城市名字的,只不过需要自己提取

    			获取方式: 
    			url = response.url
    			ss = url[url.find('=')+1 : url.find('&')]
    			city_name = urllib.unquote(ss)  -->'白沙'

    			在python2中还要decode操作
    			city_name = city_name.decode('utf-8')




6.手写--Csv管道,MongoDB管道,redis管道
	
	----------------------------------------------------------
	操作:
	    1.在pipelines.py中编写三个管道,并按执行顺序在setting.py中开启管道:
	    	from scrapy.exporters import CsvItemExporter
			import pymongo
			import redis


			# CSV的管道
			class AqiCsvPipeline(object):
			    def open_spider(self, spider):
			        self.file = open('aqi.csv', 'w')
			        # 1.生成读写器
			        self.writer = CsvItemExporter(self.file)
			        # 2.启动
			        self.writer.start_exporting()

			    def process_item(self, item, spider):
			        # 3.直接存储到 csv使用读写器
			        self.writer.export_item(item)
			        return item

			    def close_spider(self, spider):
			        # 4.关闭读写器
			        self.writer.finish_exporting()
			        self.file.close()


			# mongodb的管道
			class AqiMongoPipeline(object):
			    def open_spider(self, spider):
			        # 连接mongodb
			        self.client = pymongo.MongoClient(host='127.0.0.1',port=27017)
			        # 数据库起名字
			        self.db = self.client['AQImongo']
			        self.collection = self.db['aqi']  # 建表

			    def process_item(self, item, spider):
			        # 3.直接存储到 csv使用读写器
			        self.collection.insert(dict(item))
			        return item


			# redis的管道
			class AqiRedisPipeline(object):
			    def open_spider(self, spider):
			        # 连接mongodb
			        self.client = redis.Redis(host='127.0.0.1',port=6379)

			    def process_item(self, item, spider):

			        # 字符串
			        content = json.dumps(dict(item))
			        self.client.lpush('AQI_list', content)
			        return item

		2.开启管道:

			ITEM_PIPELINES = {
			   'AQI.pipelines.AqiDataSourcePipeline': 300,
			    'AQI.pipelines.AqiJsonPipeline':400,
			    'AQI.pipelines.AqiCsvPipeline':500,
			    'AQI.pipelines.AqiMongoPipeline':600,
			    'AQI.pipelines.AqiRedisPipeline':700
			}	        
		----------------------------------------------------------------	
		注:
		1.上面完成操作之后,开启爬虫: (一定要开启aqi_spider 而不是 aqi_crawl)
		2.要保证MongoDB 和 redis 是开启的状态:
			windows下启动MongoDB: d:\mongodb\bin>mongod -dbpath "d:\mongodb\data\db"

		3.每个管道连接数据库的方式不同,要分别对待,但是存储的时候有的需要转换
			csv     -->直接储存 item 
			mongodb -->dict(item)
			redis   -->json.dumps(dict(item))
		4.数据库的操作要先连接,注意 ip 和端口的填写
		5.MongoDB操作: 
			show dbs
			use AQImongo
			show collections
			db.aqi.count()




7.redis分布式布局
	
	spider.py:
		1.导库,RedisCrawlSpider
			from scrapy_redis.spiders import RedisSpider

		2.继承关系
			class AqiSpiderSpider(RedisSpider):

		3.redis_key = 
			将start_urls = [''] 注释,然后
			redis_key = 'aqi_spider:start_urls'


	setting.py:
		1.启用组件的调度器
		2.启用去重的组件
		3.是否可以暂停下载	
		4.开启redis的管道
		5.设置ip 和 port




	注:	
	1.分布式爬虫 只要一开启,就会持续等待任务!
	2.不使用分布式,也可以用分布式管道

	3.开启分布式爬虫之后,如果一个任务爬取完成之后,机器处于等待任务状态,
		此时如果继续在redis-cli中输入: lpush spider:start_urls 'https:www.aqistudy.cn/historydata/'
		再跑一边,爬虫会抛出 下列提示:
		no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)

		意思是上面的url 我已经爬取过了(这是scrapy-redis去重造成的),不信的话你可以看看redis数据库中
		存储着已经发送过的request的指纹! 此时,如果删除这个指纹,那么又可以继续爬取了!分布式就是靠这个指纹来识别request请求是否是重复的,从而达到去重的目的!




------------------------------------------------------------
小结:
	1.新建爬虫的时候,要 scrapy genspider xx  xx.com (这个域名不一定对)
	所以在新建号爬虫的时候需要再次进去改一下,改成能正常访问的

	2.关于动态加载页面:
		1.首先有一个基本的页面比如:
			https://www.aqistudy.cn/historydata/

		2.然后如果想进入到下一个页面,必须要拼接地址,那么这个地址在哪呢?
			就在标签里的  'href='里面
			浏览器接收到请求之后,解析到 href='daydata.php?city=北京&month=2013-12',会自动拼接地址:
			https://www.aqistudy.cn/historydata/  + daydata.php?city=北京&month=2013-12

			从而加载页面数据!




-------------------------------------------=-----------------------















































































































































































